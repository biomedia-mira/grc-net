{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3777e48",
   "metadata": {},
   "source": [
    "# Geo-Radio Classification\n",
    "\n",
    "This notebook is used to train the classification model on the ASOCA Dataset.\n",
    "The notebook assumes the previous scripts have ran, which have saved the following models:\n",
    "- a segmentation model trained using anatomix, currently pointing to `saved_models/segmentation/anatomix_trained_MM-WHS.pth`\n",
    "- a registration model trained using Atlas-ISTN, currently pointing to `output/mm-whs/full-stn/train/model/stn.pt`\n",
    "- an atlas labelmap created using Atlas-ISTN, currently pointing to `output/mm-whs/full-stn/train/model/atlas_labelmap_final.nii.gz`\n",
    "\n",
    "If you have run the `anatomix-fine-tuning.py` and the `atlas-istn-anatomix.py` files, these will automatically be generated for you.\n",
    "\n",
    "This model will segment the ASOCA images as directed by the config CSV file `data/config/inference.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7117799",
   "metadata": {},
   "source": [
    "# Imports and Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"anatomix\")\n",
    "\n",
    "import torch\n",
    "from monai.data import ThreadDataLoader, CacheDataset\n",
    "from monai.transforms import Lambdad\n",
    "from nets.stn import FullSTN3D\n",
    "from img.datasets import ImageSegmentationOneHotDataset\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2213162",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = (2.0, 2.0, 2.0)\n",
    "crop_size = (96, 96, 96)\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7281424",
   "metadata": {},
   "source": [
    "# Class Mapping \n",
    "*(should match \"class_mapping\" in `data/config/config.json`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "        1: \"myocardium\",\n",
    "        2: \"left atrium\",\n",
    "        3: \"left ventricle\",\n",
    "        4: \"right atrium\",\n",
    "        5: \"right ventricle\",\n",
    "        6: \"aorta\",\n",
    "        7: \"pulmonary artery\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0139fa8",
   "metadata": {},
   "source": [
    "# Load STN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_path = \"output/mm-whs/full-stn/train/model/stn.pt\"\n",
    "stn = FullSTN3D(input_size=crop_size, input_channels=2*(num_classes-1), device=device).to(device)\n",
    "stn.load_state_dict(torch.load(stn_path))\n",
    "stn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9f652",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_base = ImageSegmentationOneHotDataset(\"data/config/inference.csv\", \n",
    "                                            num_classes, crop_size, spacing, \n",
    "                                            normalizer=Lambdad(keys=[\"image\"], func=lambda x: x), binarize=0, augmentation=False)\n",
    "dataset_test = CacheDataset(data=dataset_test_base, transform=None, cache_rate=1.0, num_workers=4)\n",
    "dataset_test.get_sample = dataset_test_base.get_sample\n",
    "dataloader_test = ThreadDataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d39de6",
   "metadata": {},
   "source": [
    "# Extract radiomic and deformation data\n",
    "This loop will iterate over each CT test volume in `inference.csv`, creating features per volume in a list.\n",
    "For each test volume, we store the following features for downstream classification:\n",
    "- **label**, described as either \"Diseased\" or \"Healthy\" (obtained by parsing the file name).\n",
    "- **struct_disp**, a dictionary keyed per substructure storing the respective deformation displacement field.\n",
    "- **radiomics**, a dictionary keyed per substructure storing the respective radiomics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "radiomics_settings = {\n",
    "    'binWidth': 25,\n",
    "    'resampledPixelSpacing': None,\n",
    "    'interpolator': sitk.sitkLinear,\n",
    "    'verbose': False\n",
    "}\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(**radiomics_settings)\n",
    "\n",
    "atlas_label_itk = sitk.ReadImage(\"output/mm-whs/full-stn/train/model/atlas_labelmap_final.nii.gz\")\n",
    "\n",
    "arr_lab = sitk.GetArrayFromImage(atlas_label_itk)\n",
    "if arr_lab.ndim == 4:  # already one‐hot in last dim\n",
    "    atlas_label = (\n",
    "        torch.from_numpy(arr_lab)\n",
    "        .permute(3, 0, 1, 2)\n",
    "        .unsqueeze(0)\n",
    "        .float()\n",
    "        .to(device)\n",
    "    )\n",
    "else:\n",
    "    labels_int = torch.from_numpy(arr_lab).long()\n",
    "    one_hot    = torch.nn.functional.one_hot(labels_int, num_classes=num_classes)\n",
    "    atlas_label = one_hot.permute(3, 0, 1, 2).unsqueeze(0).float().to(device)\n",
    "\n",
    "example_batch = next(iter(dataloader_test))\n",
    "batch_size = example_batch[\"image\"].size(0)\n",
    "atlas_label = atlas_label.repeat(batch_size, 1, 1, 1, 1)\n",
    "\n",
    "# Precompute identity grid once (for displacement = T – grid)\n",
    "identity_grid = stn.grid.unsqueeze(0)\n",
    "identity_grid = stn.move_grid_dims(identity_grid)\n",
    "identity_grid = identity_grid.repeat(batch_size, 1, 1, 1, 1).to(device)\n",
    "\n",
    "subjects = []\n",
    "\n",
    "for batch in tqdm(dataloader_test, desc=\"extracting features\"):\n",
    "    image_tensor = batch[\"image\"].to(device)\n",
    "    label_onehot  = batch[\"labelmap\"].to(device)\n",
    "    fname         = batch[\"fname\"][0]\n",
    "\n",
    "    img_type = \"Diseased\" if \"Diseased\" in fname else \"Normal\"\n",
    "\n",
    "    # Run STN to get full warp grid T\n",
    "    src = label_onehot[:, 1:, ...]\n",
    "    tgt = atlas_label[:, 1:, ...]\n",
    "    _   = stn(torch.cat((src, tgt), dim=1))\n",
    "\n",
    "    T = stn.get_T()\n",
    "    full_disp = T - identity_grid\n",
    "    disp_np = full_disp[0].detach().cpu().numpy()\n",
    "\n",
    "    # per‐structure displacement\n",
    "    struct_disp = {}\n",
    "    for L in class_mapping.keys():\n",
    "        maskL = label_onehot[0, L].bool().cpu().numpy()  \n",
    "        disp_vox = disp_np[maskL]\n",
    "        struct_disp[L] = disp_vox\n",
    "\n",
    "    # per‐structure radiomics\n",
    "    img_np = image_tensor[0, 0].detach().cpu().numpy()\n",
    "    sitk_img = sitk.GetImageFromArray(img_np)\n",
    "    sitk_img.SetSpacing(spacing)\n",
    "\n",
    "    radiomics = {}\n",
    "    for L, name in class_mapping.items():\n",
    "        mask_np = label_onehot[0, L].cpu().numpy().astype(np.uint8)\n",
    "        if mask_np.sum() == 0:\n",
    "            # No voxels, so store an array of nans with length = len(SEMANTIC_FEATURES)\n",
    "            radiomics[L] = np.full((len(SEMANTIC_FEATURES),), np.nan, dtype=float)\n",
    "        else:\n",
    "            sitk_mask = sitk.GetImageFromArray(mask_np)\n",
    "            sitk_mask.CopyInformation(sitk_img)\n",
    "            result = extractor.execute(sitk_img, sitk_mask)\n",
    "            # get only original features (features derived from the unfiltered CT image)\n",
    "            # of the original subset, the radiomic features are:\n",
    "            # – First-order statistics\n",
    "            # – Shape descriptors (3D)\n",
    "            # – GLCM (Gray Level Co-occurrence Matrix)\n",
    "            # – GLRLM (Gray Level Run Length Matrix)\n",
    "            # – GLSZM (Gray Level Size Zone Matrix)\n",
    "            # – NGTDM (Neighbouring Gray Tone Difference Matrix)\n",
    "            # – GLDM (Gray Level Dependence Matrix)\n",
    "            SEMANTIC_FEATURES = sorted([k for k in result.keys() if k.startswith(\"original_\")])\n",
    "\n",
    "            feats = []\n",
    "            for feat_name in SEMANTIC_FEATURES:\n",
    "                val = result.get(feat_name, float(\"nan\"))\n",
    "                feats.append(float(val))\n",
    "            radiomics[L] = np.array(feats, dtype=float)\n",
    "\n",
    "    # Collect everything into a single dict for this subject\n",
    "    subject_data = {\n",
    "        \"fname\":     fname,\n",
    "        \"label\":     img_type,\n",
    "        \"full_disp\": disp_np,       # [D,H,W,3]\n",
    "        \"struct_disp\": struct_disp, # dict L->(n_vox_L,3)\n",
    "        \"radiomics\":   radiomics    # dict L->(len(SEMANTIC_FEATURES),)\n",
    "    }\n",
    "    subjects.append(subject_data)\n",
    "\n",
    "# After this loop, `subjects` is a list of length N (test cases),\n",
    "# and each `subjects[i]` contains all the deformation + radiomics for that case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4f2f4",
   "metadata": {},
   "source": [
    "# MLP classification\n",
    "Here, we perform hyperparameter optimisation using `optuna` to find the best classification model for the diseased data.\n",
    "We perform 5-fold stratified cross-validation over 3 seeds to achieve confidence in the low volume of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.ops import MLP\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "MAX_DEF_PC = 3\n",
    "\n",
    "rows = []\n",
    "for subj in subjects:\n",
    "    row = {}\n",
    "    \n",
    "    for L, name in class_mapping.items():\n",
    "        disp_vox = subj[\"struct_disp\"][L]\n",
    "        n_vox = disp_vox.shape[0]\n",
    "        \n",
    "        if n_vox < 1:\n",
    "            # no voxels → all zeros\n",
    "            evr = np.zeros(MAX_DEF_PC, dtype=float)\n",
    "        else:\n",
    "            u, s, vh = np.linalg.svd(disp_vox, full_matrices=False)\n",
    "\n",
    "            # Store top MAX_DEF_PC singular values, pad with zeros if needed\n",
    "            evr = np.zeros(MAX_DEF_PC, dtype=float)\n",
    "            n_comp = min(len(s), MAX_DEF_PC)\n",
    "            evr[:n_comp] = s[:n_comp]\n",
    "        \n",
    "        # store under def_pc1_<name> … def_pc5_<name>\n",
    "        for pc_idx in range(MAX_DEF_PC):\n",
    "            col = f\"def_pc{pc_idx+1}_{name}\"\n",
    "            row[col] = float(evr[pc_idx])\n",
    "    \n",
    "    #Radiomics (semantic features) per structure\n",
    "    for L, name in class_mapping.items():\n",
    "        rad_vec = subj[\"radiomics\"][L]  # shape = (len(SEMANTIC_FEATURES),) or all-nan\n",
    "        for idx, feat_name in enumerate(SEMANTIC_FEATURES):\n",
    "            col = f\"{feat_name}_{name}\"\n",
    "            row[col] = float(rad_vec[idx])\n",
    "    \n",
    "    row[\"label\"] = subj[\"label\"]  # \"Normal\" or \"Diseased\"\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df_full = pd.DataFrame.from_records(rows)\n",
    "print(df_full.isna().any()[lambda x: x])\n",
    "print(df_full.head(4))\n",
    "print(\"Shape of df_full:\", df_full.shape)\n",
    "\n",
    "y_global = np.array(df_full[\"label\"])\n",
    "N = len(y_global)\n",
    "\n",
    "seeds = [10, 101, 202]\n",
    "\n",
    "def get_folds(seed):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    return list(skf.split(np.zeros((N, 1)), y_global))\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 8, 512, step=8)\n",
    "    lr           = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    dropout      = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    num_layers   = trial.suggest_int(\"num_layers\", 1, 12)\n",
    "    num_epochs   = trial.suggest_int(\"num_epochs\", 100, 400, step=25)\n",
    "\n",
    "    def_pc_amt = trial.suggest_int(\"def_pc_amt\", 1, MAX_DEF_PC)\n",
    "\n",
    "    selected_cols = []\n",
    "    for L, name in class_mapping.items():\n",
    "        for i in range(def_pc_amt):\n",
    "            selected_cols.append(f\"def_pc{i+1}_{name}\")\n",
    "        for i, feat_name in enumerate(SEMANTIC_FEATURES):\n",
    "            selected_cols.append(f\"{feat_name}_{name}\")\n",
    "\n",
    "    X_df = df_full[selected_cols]                   # shape (N_samples, def_pc_amt*7 + F_sem*7)\n",
    "    X_np = X_df.to_numpy(dtype=np.float32)           # numpy array (N_samples, D_trial)\n",
    "    y_np = y_global.copy()                           # numpy array (N_samples,), strings \"Normal\"/\"Diseased\"\n",
    "    N_samples = len(y_np)\n",
    "\n",
    "    X_all = torch.from_numpy(X_np).float()\n",
    "    y_all = torch.from_numpy((y_np == \"Diseased\").astype(np.float32)).to(device)\n",
    "\n",
    "    seed_means = []\n",
    "    per_seed_fold_metrics = []\n",
    "\n",
    "    for s in seeds:\n",
    "        random.seed(s)\n",
    "        np.random.seed(s)\n",
    "        torch.manual_seed(s)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=s)\n",
    "        # pass a dummy X of shape (N_samples, 1) because StratifiedKFold only uses y\n",
    "        folds = list(skf.split(np.zeros((N_samples, 1)), y_np))\n",
    "\n",
    "        fold_accuracy_list = []\n",
    "        fold_metrics_list = []\n",
    "\n",
    "        for (train_idx, val_idx) in folds:\n",
    "            X_train = X_all[train_idx]\n",
    "            y_train = y_all[train_idx]\n",
    "            X_val   = X_all[val_idx]\n",
    "            y_val   = y_all[val_idx]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "            X_train = torch.from_numpy(X_train_scaled).float().to(device)\n",
    "            X_val   = torch.from_numpy(X_val_scaled).float().to(device)\n",
    "\n",
    "            # MLP\n",
    "            layers = [hidden_units] * num_layers + [1]\n",
    "            model = MLP(X_np.shape[1], layers, dropout=dropout).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(1, num_epochs + 1):\n",
    "                model.train()\n",
    "                logits = model(X_train).squeeze(1)\n",
    "                loss = criterion(logits, y_train)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(X_val).squeeze(1)\n",
    "                val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
    "                val_preds = (val_probs >= 0.5).astype(int)\n",
    "\n",
    "                y_true = y_val.cpu().numpy().astype(int)\n",
    "                y_pred = val_preds\n",
    "\n",
    "                acc  = accuracy_score(y_true, y_pred)\n",
    "                prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "                rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "                f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "                cm   = confusion_matrix(y_true, y_pred)\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "            fold_accuracy_list.append(acc)\n",
    "            fold_metrics_list.append({\n",
    "                \"accuracy\":        acc,\n",
    "                \"precision\":       prec,\n",
    "                \"recall\":          rec,\n",
    "                \"f1\":              f1,\n",
    "                \"sensitivity\":     sensitivity,\n",
    "                \"specificity\":     specificity,\n",
    "                \"confusion_matrix\": cm\n",
    "            })\n",
    "\n",
    "        seed_mean_acc = np.mean(fold_accuracy_list)\n",
    "        seed_means.append(seed_mean_acc)\n",
    "        per_seed_fold_metrics.append(fold_metrics_list)\n",
    "\n",
    "    trial.set_user_attr(\"seed_means\", seed_means)\n",
    "    trial.set_user_attr(\"per_seed_fold_metrics\", per_seed_fold_metrics)\n",
    "    trial.set_user_attr(\"hyperparams\", {\n",
    "        \"hidden_units\": hidden_units,\n",
    "        \"lr\":            lr,\n",
    "        \"dropout\":       dropout,\n",
    "        \"num_layers\":    num_layers,\n",
    "        \"num_epochs\":    num_epochs,\n",
    "        \"def_pc_amt\":    def_pc_amt\n",
    "    })\n",
    "\n",
    "    return np.mean(seed_means)\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=500, show_progress_bar=True)\n",
    "\n",
    "# Tie-break on stddev\n",
    "trials = study.trials\n",
    "best_trial = max(trials, key=lambda t: (t.value, -np.std(t.user_attrs[\"seed_means\"])))\n",
    "\n",
    "best_score       = best_trial.value\n",
    "best_std_seed    = np.std(best_trial.user_attrs[\"seed_means\"])\n",
    "best_hyperparams = best_trial.user_attrs[\"hyperparams\"]\n",
    "best_fold_info   = {\n",
    "    'seeds': seeds,\n",
    "    'seed_means': best_trial.user_attrs[\"seed_means\"],\n",
    "    'per_seed_fold_metrics': best_trial.user_attrs[\"per_seed_fold_metrics\"]\n",
    "}\n",
    "\n",
    "# 7) Aggregate all metrics over seeds & folds\n",
    "all_accuracies    = []\n",
    "all_precisions    = []\n",
    "all_recalls       = []\n",
    "all_f1s           = []\n",
    "all_sensitivities = []\n",
    "all_specificities = []\n",
    "\n",
    "for seed_metrics in best_fold_info['per_seed_fold_metrics']:\n",
    "    for m in seed_metrics:\n",
    "        all_accuracies.append(m['accuracy'])\n",
    "        all_precisions.append(m['precision'])\n",
    "        all_recalls.append(m['recall'])\n",
    "        all_f1s.append(m['f1'])\n",
    "        all_sensitivities.append(m['sensitivity'])\n",
    "        all_specificities.append(m['specificity'])\n",
    "\n",
    "acc_mean,  acc_std  = np.mean(all_accuracies),    np.std(all_accuracies)\n",
    "prec_mean, prec_std = np.mean(all_precisions),   np.std(all_precisions)\n",
    "rec_mean,  rec_std  = np.mean(all_recalls),      np.std(all_recalls)\n",
    "f1_mean,   f1_std   = np.mean(all_f1s),          np.std(all_f1s)\n",
    "sens_mean, sens_std = np.mean(all_sensitivities), np.std(all_sensitivities)\n",
    "spec_mean, spec_std = np.mean(all_specificities), np.std(all_specificities)\n",
    "\n",
    "print(\"\\n=== Best Hyperparameters (by avg-seed, tie-break on lowest std) ===\")\n",
    "for k, v in best_hyperparams.items():\n",
    "    print(f\"{k:<12}: {v}\")\n",
    "print(f\"Avg of seed-means = {best_score:.4f}\")\n",
    "print(f\"Std of seed-means = {best_std_seed:.4f}\\n\")\n",
    "\n",
    "print(\"=== Aggregate Metrics over all seeds & folds (mean ± std) ===\")\n",
    "print(f\"Accuracy    : {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "print(f\"Precision   : {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "print(f\"Recall      : {rec_mean:.4f} ± {rec_std:.4f}\")\n",
    "print(f\"F1 Score    : {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "print(f\"Sensitivity : {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "print(f\"Specificity : {spec_mean:.4f} ± {spec_std:.4f}\\n\")\n",
    "\n",
    "for seed_idx, s in enumerate(best_fold_info['seeds']):\n",
    "    print(f\"--- Seed {s} (mean CV acc = {best_fold_info['seed_means'][seed_idx]:.4f}) ---\")\n",
    "    for fold_idx, m in enumerate(best_fold_info['per_seed_fold_metrics'][seed_idx], start=1):\n",
    "        print(f\"Fold {fold_idx}:\")\n",
    "        print(f\"  Accuracy    = {m['accuracy']:.4f}\")\n",
    "        print(f\"  Precision   = {m['precision']:.4f}\")\n",
    "        print(f\"  Recall      = {m['recall']:.4f}\")\n",
    "        print(f\"  F1 Score    = {m['f1']:.4f}\")\n",
    "        print(f\"  Sensitivity = {m['sensitivity']:.4f}\")\n",
    "        print(f\"  Specificity = {m['specificity']:.4f}\")\n",
    "        print(f\"  Confusion Matrix:\\n{m['confusion_matrix']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae301c7",
   "metadata": {},
   "source": [
    "# ResNet Baseline\n",
    "We provide a Resnet-50 model as an Image-only baseline to compare our model against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbc596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from monai.networks.nets import resnet\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "dataset = dataset_test\n",
    "all_indices = list(range(len(dataset)))\n",
    "\n",
    "valid_indices = []\n",
    "labels = []\n",
    "for i in all_indices:\n",
    "    fname = dataset[i]['fname']\n",
    "    if \"Diseased\" in fname:\n",
    "        valid_indices.append(i)\n",
    "        labels.append(1)\n",
    "    elif \"Normal\" in fname:\n",
    "        valid_indices.append(i)\n",
    "        labels.append(0)\n",
    "labels = np.array(labels)\n",
    "N = len(valid_indices)\n",
    "\n",
    "seeds = [10, 101, 202]\n",
    "\n",
    "def make_resnet50_3d():\n",
    "    return resnet.resnet50(\n",
    "        spatial_dims=3,\n",
    "        n_input_channels=1,\n",
    "        num_classes=1\n",
    "    )\n",
    "\n",
    "def get_folds(seed):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    return list(skf.split(np.zeros((N, 1)), labels))\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "    num_epochs = trial.suggest_categorical(\"num_epochs\", [25, 50, 100])\n",
    "\n",
    "    seed_means = []\n",
    "    per_seed_fold_metrics = []\n",
    "\n",
    "    for s in seeds:\n",
    "        random.seed(s)\n",
    "        np.random.seed(s)\n",
    "        torch.manual_seed(s)\n",
    "\n",
    "        folds = get_folds(s)\n",
    "        fold_accs = []\n",
    "        fold_metrics = []\n",
    "\n",
    "        for train_idx, val_idx in folds:\n",
    "            train_dataset_indices = [valid_indices[i] for i in train_idx]\n",
    "            val_dataset_indices = [valid_indices[i] for i in val_idx]\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=1,\n",
    "                sampler=SubsetRandomSampler(train_dataset_indices),\n",
    "                num_workers=2,\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=1,\n",
    "                sampler=SubsetRandomSampler(val_dataset_indices),\n",
    "                num_workers=2,\n",
    "                pin_memory=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            model = make_resnet50_3d().to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "            for epoch in range(1, num_epochs + 1):\n",
    "                model.train()\n",
    "                for batch in train_loader:\n",
    "                    imgs = batch['image'].float().to(device)\n",
    "                    fname = batch['fname'][0]\n",
    "                    lbl = torch.tensor(\n",
    "                        [1.0 if \"Diseased\" in fname else 0.0],\n",
    "                        device=device\n",
    "                    ).view(-1)\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model(imgs).view(-1)\n",
    "                    loss = criterion(logits, lbl)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            all_preds, all_trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    imgs = batch['image'].float().to(device)\n",
    "                    fname = batch['fname'][0]\n",
    "                    true_lbl = 1 if \"Diseased\" in fname else 0\n",
    "                    logits = model(imgs).view(-1)\n",
    "                    prob = torch.sigmoid(logits).cpu().item()\n",
    "                    pred = 1 if prob >= 0.5 else 0\n",
    "                    all_preds.append(pred)\n",
    "                    all_trues.append(true_lbl)\n",
    "\n",
    "            y_true = np.array(all_trues)\n",
    "            y_pred = np.array(all_preds)\n",
    "            acc  = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "            rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "            cm   = confusion_matrix(y_true, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            sensitivity = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "            specificity = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "\n",
    "            fold_accs.append(acc)\n",
    "            fold_metrics.append({\n",
    "                'accuracy': acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec,\n",
    "                'f1': f1,\n",
    "                'sensitivity': sensitivity,\n",
    "                'specificity': specificity,\n",
    "                'confusion_matrix': cm\n",
    "            })\n",
    "\n",
    "        seed_means.append(np.mean(fold_accs))\n",
    "        per_seed_fold_metrics.append(fold_metrics)\n",
    "\n",
    "    trial.set_user_attr(\"seed_means\", seed_means)\n",
    "    trial.set_user_attr(\"per_seed_fold_metrics\", per_seed_fold_metrics)\n",
    "    trial.set_user_attr(\"hyperparams\", {\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'num_epochs': num_epochs\n",
    "    })\n",
    "\n",
    "    return np.mean(seed_means)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# We do 3 trials for brevity, but can be increased further if you have more compute/time.\n",
    "# Else, you can reduce the number of seeds (from 3) or reduce the number of folds (from 5).\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "trials = study.trials\n",
    "best_trial = max(trials, key=lambda t: (t.value, -np.std(t.user_attrs[\"seed_means\"])))\n",
    "\n",
    "best_score       = best_trial.value\n",
    "best_std_seed    = np.std(best_trial.user_attrs[\"seed_means\"])\n",
    "best_hyperparams = best_trial.user_attrs[\"hyperparams\"]\n",
    "best_fold_info   = {\n",
    "    'seeds': seeds,\n",
    "    'seed_means': best_trial.user_attrs[\"seed_means\"],\n",
    "    'per_seed_fold_metrics': best_trial.user_attrs[\"per_seed_fold_metrics\"]\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Best Hyperparameters (by avg-seed, tie-break on lowest std) ===\")\n",
    "for k, v in best_hyperparams.items():\n",
    "    print(f\"{k:<12}: {v}\")\n",
    "print(f\"Avg of seed-means = {best_score:.4f}\")\n",
    "print(f\"Std of seed-means = {best_std_seed:.4f}\\n\")\n",
    "\n",
    "# Aggregate all metrics over seeds & folds\n",
    "all_accuracies = []\n",
    "all_precisions = []\n",
    "all_recalls    = []\n",
    "all_f1s        = []\n",
    "all_sensitivities = []\n",
    "all_specificities = []\n",
    "\n",
    "for seed_metrics in best_fold_info['per_seed_fold_metrics']:\n",
    "    for m in seed_metrics:\n",
    "        all_accuracies.append(m['accuracy'])\n",
    "        all_precisions.append(m['precision'])\n",
    "        all_recalls.append(m['recall'])\n",
    "        all_f1s.append(m['f1'])\n",
    "        all_sensitivities.append(m['sensitivity'])\n",
    "        all_specificities.append(m['specificity'])\n",
    "\n",
    "acc_mean,  acc_std  = np.mean(all_accuracies),    np.std(all_accuracies)\n",
    "prec_mean, prec_std = np.mean(all_precisions),   np.std(all_precisions)\n",
    "rec_mean,  rec_std  = np.mean(all_recalls),      np.std(all_recalls)\n",
    "f1_mean,   f1_std   = np.mean(all_f1s),          np.std(all_f1s)\n",
    "sens_mean, sens_std = np.mean(all_sensitivities), np.std(all_sensitivities)\n",
    "spec_mean, spec_std = np.mean(all_specificities), np.std(all_specificities)\n",
    "\n",
    "print(\"=== Aggregate Metrics over all seeds & folds (mean ± std) ===\")\n",
    "print(f\"Accuracy    : {acc_mean:.4f} ± {acc_std:.4f}\")\n",
    "print(f\"Precision   : {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "print(f\"Recall      : {rec_mean:.4f} ± {rec_std:.4f}\")\n",
    "print(f\"F1 Score    : {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "print(f\"Sensitivity : {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "print(f\"Specificity : {spec_mean:.4f} ± {spec_std:.4f}\\n\")\n",
    "\n",
    "for seed_idx, s in enumerate(best_fold_info['seeds']):\n",
    "    print(f\"--- Seed {s} (mean CV acc = {best_fold_info['seed_means'][seed_idx]:.4f}) ---\")\n",
    "    for fold_idx, m in enumerate(best_fold_info['per_seed_fold_metrics'][seed_idx], start=1):\n",
    "        print(f\"Fold {fold_idx}:\")\n",
    "        print(f\"  Accuracy    = {m['accuracy']:.4f}\")\n",
    "        print(f\"  Precision   = {m['precision']:.4f}\")\n",
    "        print(f\"  Recall      = {m['recall']:.4f}\")\n",
    "        print(f\"  F1 Score    = {m['f1']:.4f}\")\n",
    "        print(f\"  Sensitivity = {m['sensitivity']:.4f}\")\n",
    "        print(f\"  Specificity = {m['specificity']:.4f}\")\n",
    "        print(f\"  Confusion Matrix:\\n{m['confusion_matrix']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRCNET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
